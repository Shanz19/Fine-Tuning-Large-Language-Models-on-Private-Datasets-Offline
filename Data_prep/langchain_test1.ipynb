{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAbMmK9VjG8x",
        "outputId": "a95681dc-5882-4b62-e60a-e849345468b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Requirement already satisfied: torch in .\\faye2\\lib\\site-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision in .\\faye2\\lib\\site-packages (0.18.0+cu121)\n",
            "Requirement already satisfied: torchaudio in .\\faye2\\lib\\site-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: filelock in .\\faye2\\lib\\site-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in .\\faye2\\lib\\site-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in .\\faye2\\lib\\site-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in .\\faye2\\lib\\site-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in .\\faye2\\lib\\site-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in .\\faye2\\lib\\site-packages (from torch) (2024.3.1)\n",
            "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in .\\faye2\\lib\\site-packages (from torch) (2021.4.0)\n",
            "Requirement already satisfied: numpy in .\\faye2\\lib\\site-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in .\\faye2\\lib\\site-packages (from torchvision) (10.3.0)\n",
            "Requirement already satisfied: intel-openmp==2021.* in .\\faye2\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
            "Requirement already satisfied: tbb==2021.* in .\\faye2\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.12.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in .\\faye2\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in .\\faye2\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTRv0tZqjOTh",
        "outputId": "f323c761-5cb8-4c7e-d806-5cf8bc49a186"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in .\\faye2\\lib\\site-packages (0.1.17)"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Requirement already satisfied: bitsandbytes in .\\faye2\\lib\\site-packages (0.43.1)\n",
            "Requirement already satisfied: chromadb in .\\faye2\\lib\\site-packages (0.5.0)\n",
            "Requirement already satisfied: sentence_transformers in .\\faye2\\lib\\site-packages (2.7.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in .\\faye2\\lib\\site-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in .\\faye2\\lib\\site-packages (from langchain) (2.0.29)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in .\\faye2\\lib\\site-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in .\\faye2\\lib\\site-packages (from langchain) (0.6.5)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in .\\faye2\\lib\\site-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.36 in .\\faye2\\lib\\site-packages (from langchain) (0.0.36)\n",
            "Requirement already satisfied: langchain-core<0.2.0,>=0.1.48 in .\\faye2\\lib\\site-packages (from langchain) (0.1.48)\n",
            "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in .\\faye2\\lib\\site-packages (from langchain) (0.0.1)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in .\\faye2\\lib\\site-packages (from langchain) (0.1.52)\n",
            "Requirement already satisfied: numpy<2,>=1 in .\\faye2\\lib\\site-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3,>=1 in .\\faye2\\lib\\site-packages (from langchain) (2.7.1)\n",
            "Requirement already satisfied: requests<3,>=2 in .\\faye2\\lib\\site-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in .\\faye2\\lib\\site-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: torch in .\\faye2\\lib\\site-packages (from bitsandbytes) (2.3.0+cu121)\n",
            "Requirement already satisfied: build>=1.0.3 in .\\faye2\\lib\\site-packages (from chromadb) (1.2.1)\n",
            "Requirement already satisfied: chroma-hnswlib==0.7.3 in .\\faye2\\lib\\site-packages (from chromadb) (0.7.3)\n",
            "Requirement already satisfied: fastapi>=0.95.2 in .\\faye2\\lib\\site-packages (from chromadb) (0.110.3)\n",
            "Requirement already satisfied: uvicorn[standard]>=0.18.3 in .\\faye2\\lib\\site-packages (from chromadb) (0.29.0)\n",
            "Requirement already satisfied: posthog>=2.4.0 in .\\faye2\\lib\\site-packages (from chromadb) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in .\\faye2\\lib\\site-packages (from chromadb) (4.11.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in .\\faye2\\lib\\site-packages (from chromadb) (1.17.3)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in .\\faye2\\lib\\site-packages (from chromadb) (1.24.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in .\\faye2\\lib\\site-packages (from chromadb) (1.24.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in .\\faye2\\lib\\site-packages (from chromadb) (0.45b0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in .\\faye2\\lib\\site-packages (from chromadb) (1.24.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in .\\faye2\\lib\\site-packages (from chromadb) (0.15.2)\n",
            "Requirement already satisfied: pypika>=0.48.9 in .\\faye2\\lib\\site-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in .\\faye2\\lib\\site-packages (from chromadb) (4.66.2)\n",
            "Requirement already satisfied: overrides>=7.3.1 in .\\faye2\\lib\\site-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in .\\faye2\\lib\\site-packages (from chromadb) (6.4.0)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in .\\faye2\\lib\\site-packages (from chromadb) (1.63.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in .\\faye2\\lib\\site-packages (from chromadb) (4.1.2)\n",
            "Requirement already satisfied: typer>=0.9.0 in .\\faye2\\lib\\site-packages (from chromadb) (0.12.3)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in .\\faye2\\lib\\site-packages (from chromadb) (29.0.0)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in .\\faye2\\lib\\site-packages (from chromadb) (4.1.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in .\\faye2\\lib\\site-packages (from chromadb) (3.10.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in .\\faye2\\lib\\site-packages (from sentence_transformers) (4.39.1)\n",
            "Requirement already satisfied: scikit-learn in .\\faye2\\lib\\site-packages (from sentence_transformers) (1.4.2)\n",
            "Requirement already satisfied: scipy in .\\faye2\\lib\\site-packages (from sentence_transformers) (1.13.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in .\\faye2\\lib\\site-packages (from sentence_transformers) (0.22.2)\n",
            "Requirement already satisfied: Pillow in .\\faye2\\lib\\site-packages (from sentence_transformers) (10.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in .\\faye2\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in .\\faye2\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in .\\faye2\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in .\\faye2\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in .\\faye2\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: packaging>=19.1 in .\\faye2\\lib\\site-packages (from build>=1.0.3->chromadb) (23.2)\n",
            "Requirement already satisfied: pyproject_hooks in .\\faye2\\lib\\site-packages (from build>=1.0.3->chromadb) (1.1.0)\n",
            "Requirement already satisfied: colorama in .\\faye2\\lib\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in .\\faye2\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in .\\faye2\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in .\\faye2\\lib\\site-packages (from fastapi>=0.95.2->chromadb) (0.37.2)\n",
            "Requirement already satisfied: filelock in .\\faye2\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in .\\faye2\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2024.3.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in .\\faye2\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: certifi>=14.05.14 in .\\faye2\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2024.2.2)\n",
            "Requirement already satisfied: six>=1.9.0 in .\\faye2\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in .\\faye2\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in .\\faye2\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.29.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in .\\faye2\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in .\\faye2\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in .\\faye2\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in .\\faye2\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: coloredlogs in .\\faye2\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in .\\faye2\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
            "Requirement already satisfied: protobuf in .\\faye2\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (4.25.3)\n",
            "Requirement already satisfied: sympy in .\\faye2\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in .\\faye2\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\n",
            "Requirement already satisfied: importlib-metadata<=7.0,>=6.0 in .\\faye2\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (7.0.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in .\\faye2\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.63.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.24.0 in .\\faye2\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.24.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.24.0 in .\\faye2\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.24.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.45b0 in .\\faye2\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.45b0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation==0.45b0 in .\\faye2\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.45b0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.45b0 in .\\faye2\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.45b0)\n",
            "Requirement already satisfied: opentelemetry-util-http==0.45b0 in .\\faye2\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.45b0)\n",
            "Requirement already satisfied: setuptools>=16.0 in .\\faye2\\lib\\site-packages (from opentelemetry-instrumentation==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (65.5.0)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in .\\faye2\\lib\\site-packages (from opentelemetry-instrumentation==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: asgiref~=3.0 in .\\faye2\\lib\\site-packages (from opentelemetry-instrumentation-asgi==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
            "Requirement already satisfied: monotonic>=1.5 in .\\faye2\\lib\\site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
            "Requirement already satisfied: backoff>=1.10.0 in .\\faye2\\lib\\site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in .\\faye2\\lib\\site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in .\\faye2\\lib\\site-packages (from pydantic<3,>=1->langchain) (2.18.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in .\\faye2\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in .\\faye2\\lib\\site-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in .\\faye2\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: networkx in .\\faye2\\lib\\site-packages (from torch->bitsandbytes) (3.3)\n",
            "Requirement already satisfied: jinja2 in .\\faye2\\lib\\site-packages (from torch->bitsandbytes) (3.1.3)\n",
            "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in .\\faye2\\lib\\site-packages (from torch->bitsandbytes) (2021.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in .\\faye2\\lib\\site-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (2024.4.28)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in .\\faye2\\lib\\site-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.4.3)\n",
            "Requirement already satisfied: click>=8.0.0 in .\\faye2\\lib\\site-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in .\\faye2\\lib\\site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in .\\faye2\\lib\\site-packages (from typer>=0.9.0->chromadb) (13.7.1)\n",
            "Requirement already satisfied: h11>=0.8 in .\\faye2\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\n",
            "Requirement already satisfied: httptools>=0.5.0 in .\\faye2\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in .\\faye2\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
            "Requirement already satisfied: watchfiles>=0.13 in .\\faye2\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
            "Requirement already satisfied: websockets>=10.4 in .\\faye2\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (12.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in .\\faye2\\lib\\site-packages (from scikit-learn->sentence_transformers) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in .\\faye2\\lib\\site-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in .\\faye2\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in .\\faye2\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in .\\faye2\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: zipp>=0.5 in .\\faye2\\lib\\site-packages (from importlib-metadata<=7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.18.1)\n",
            "Requirement already satisfied: intel-openmp==2021.* in .\\faye2\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->bitsandbytes) (2021.4.0)\n",
            "Requirement already satisfied: tbb==2021.* in .\\faye2\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->bitsandbytes) (2021.12.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in .\\faye2\\lib\\site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in .\\faye2\\lib\\site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (2.17.2)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in .\\faye2\\lib\\site-packages (from starlette<0.38.0,>=0.37.2->fastapi>=0.95.2->chromadb) (4.3.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in .\\faye2\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in .\\faye2\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in .\\faye2\\lib\\site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in .\\faye2\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in .\\faye2\\lib\\site-packages (from anyio<5,>=3.4.0->starlette<0.38.0,>=0.37.2->fastapi>=0.95.2->chromadb) (1.3.1)\n",
            "Requirement already satisfied: pyreadline3 in .\\faye2\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb) (3.4.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in .\\faye2\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in .\\faye2\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.0)\n"
          ]
        }
      ],
      "source": [
        "# !pip install langchain bitsandbytes chromadb sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFucC29SjlBW",
        "outputId": "23710cf4-5b2b-428d-ef0f-f356185c5de3"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/ggerganov/llama.cpp.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugj9zNGvHlKP",
        "outputId": "7023325c-6c4d-4e78-fd1d-765a93afe9ef"
      },
      "outputs": [],
      "source": [
        "# %cd llama.cpp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uudg2E7FIYu7",
        "outputId": "f6389a23-9610-44c3-98ec-2612e8f9caf8"
      },
      "outputs": [],
      "source": [
        "# !make LLAMA_CUBLAS=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GXeRcsK-jSZp"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "f:\\shanika\\faye3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from torch import cuda, bfloat16\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoTokenizer\n",
        "from time import time\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.llms import LlamaCpp\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DI1C41H4kw29"
      },
      "outputs": [],
      "source": [
        "template =  \"\"\"Use the following pieces of context to answer the question at the end.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "{context}\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_BtQGs-ljsb",
        "outputId": "a19d1d38-8fa9-4d1f-c33f-9995f49e53f6"
      },
      "outputs": [],
      "source": [
        "# !wget https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q8_0.gguf?download=true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXpSqLBcKcOn",
        "outputId": "db231bc3-ffb4-420b-c1fb-d6f8389c8450"
      },
      "outputs": [],
      "source": [
        "# !./main -m llama-2-7b-chat.Q8_0.gguf?download=true.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJuQcTsULajE",
        "outputId": "ad6ad607-a48b-47a3-885e-df3a14366683"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: llama-cpp-python in .\\faye2\\lib\\site-packages (0.2.68)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in .\\faye2\\lib\\site-packages (from llama-cpp-python) (4.11.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in .\\faye2\\lib\\site-packages (from llama-cpp-python) (1.26.4)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in .\\faye2\\lib\\site-packages (from llama-cpp-python) (5.6.3)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in .\\faye2\\lib\\site-packages (from llama-cpp-python) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in .\\faye2\\lib\\site-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "# !pip install llama-cpp-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "miamMcuOk_1l",
        "outputId": "0bfa8074-8e9e-4852-e4f7-0bf06d3fd1bb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_model_loader: loaded meta data with 22 key-value pairs and 723 tensors from f:\\Aakash\\models\\Meta-Llama-3-70B-Instruct.Q5_K_S.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = hub\n",
            "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 8192\n",
            "llama_model_loader: - kv   5:                          llama.block_count u32              = 80\n",
            "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 28672\n",
            "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 64\n",
            "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  12:                          general.file_type u32              = 16\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,128256]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128001\n",
            "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
            "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:  161 tensors\n",
            "llama_model_loader: - type q5_K:  561 tensors\n",
            "llama_model_loader: - type q6_K:    1 tensors\n",
            "llm_load_vocab: missing pre-tokenizer type, using: 'default'\n",
            "llm_load_vocab:                                             \n",
            "llm_load_vocab: ************************************        \n",
            "llm_load_vocab: GENERATION QUALITY WILL BE DEGRADED!        \n",
            "llm_load_vocab: CONSIDER REGENERATING THE MODEL             \n",
            "llm_load_vocab: ************************************        \n",
            "llm_load_vocab:                                             \n",
            "llm_load_vocab: special tokens definition check successful ( 256/128256 ).\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 128256\n",
            "llm_load_print_meta: n_merges         = 280147\n",
            "llm_load_print_meta: n_ctx_train      = 8192\n",
            "llm_load_print_meta: n_embd           = 8192\n",
            "llm_load_print_meta: n_head           = 64\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_layer          = 80\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 8\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 28672\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 500000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 70B\n",
            "llm_load_print_meta: model ftype      = Q5_K - Small\n",
            "llm_load_print_meta: model params     = 70.55 B\n",
            "llm_load_print_meta: model size       = 45.31 GiB (5.52 BPW) \n",
            "llm_load_print_meta: general.name     = hub\n",
            "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
            "llm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
            "llm_load_tensors: ggml ctx size =    0.74 MiB\n",
            "llm_load_tensors: offloading 12 repeating layers to GPU\n",
            "llm_load_tensors: offloaded 12/81 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size = 46395.86 MiB\n",
            "llm_load_tensors:      CUDA0 buffer size =  6732.75 MiB\n",
            "...................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 4096\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:  CUDA_Host KV buffer size =  1088.00 MiB\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =   192.00 MiB\n",
            "llama_new_context_with_model: KV self size  = 1280.00 MiB, K (f16):  640.00 MiB, V (f16):  640.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.49 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =  1104.45 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =    24.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 2566\n",
            "llama_new_context_with_model: graph splits = 752\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "Model metadata: {'general.name': 'hub', 'general.architecture': 'llama', 'llama.block_count': '80', 'llama.vocab_size': '128256', 'llama.context_length': '8192', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '8192', 'llama.feed_forward_length': '28672', 'llama.attention.head_count': '64', 'tokenizer.ggml.eos_token_id': '128001', 'general.file_type': '16', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '500000.000000', 'tokenizer.ggml.model': 'gpt2', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\"}\n",
            "Using gguf chat template: {% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n",
            "\n",
            "'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "' }}\n",
            "Using chat eos_token: <|end_of_text|>\n",
            "Using chat bos_token: <|begin_of_text|>\n"
          ]
        }
      ],
      "source": [
        "\n",
        "n_gpu_layers = 12  # Change this value based on your model and your GPU VRAM pool.\n",
        "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
        "\n",
        "# Loading model,\n",
        "llm = LlamaCpp(\n",
        "    model_path=\"f:Meta-Llama-3-70B-Instruct.Q5_K_S.gguf\",\n",
        "    max_tokens=1024,\n",
        "    n_gpu_layers=n_gpu_layers,\n",
        "    n_batch=n_batch,\n",
        "    callback_manager = CallbackManager([StreamingStdOutCallbackHandler()]),\n",
        "    verbose=True,\n",
        "    n_ctx=4096, # Context window\n",
        "    stop = ['USER:'], # Dynamic stopping when such token is detected.\n",
        "    temperature = 0.4,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "T7bql6Lgm5Ld"
      },
      "outputs": [],
      "source": [
        "import PyPDF2\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "file_path = \"F:/shanika/CMS-15B_LINK_SyRS_1.4_solved.pdf\"  # Use forward slashes or raw string\n",
        "\n",
        "with open(file_path, \"rb\") as file:\n",
        "    pdf_reader = PyPDF2.PdfReader(file)\n",
        "    data = []\n",
        "    for page in pdf_reader.pages:\n",
        "        data.append(page.extract_text())\n",
        "# Load and process the text\n",
        "# loader = TextLoader('F:\\shanika\\L&T_ft.txt', encoding=\"UTF-8\")\n",
        "# documents = loader.load()\n",
        "\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "texts = text_splitter.create_documents(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "loader = TextLoader('F:\\shanika\\Document.rtf', encoding=\"UTF-8\")\n",
        "documents = loader.load()\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "texts = text_splitter.create_documents(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "o1ppEg-ynnee"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "f:\\shanika\\faye3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "model_name = \"BAAI/bge-large-en-v1.5\"\n",
        "model_kwargs = {\"device\": \"cuda\"}\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "j9fFu11Cn5O1"
      },
      "outputs": [],
      "source": [
        "#for pdf\n",
        "persist_directory = 'db'\n",
        "vectordb = Chroma.from_documents(documents=texts, embedding=embeddings, persist_directory=persist_directory)\n",
        "# vectordb = Chroma.from_documents(documents=all_splits, embedding=embeddings, persist_directory=\"chroma_db\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "#fot txt\n",
        "persist_directory = 'db1'\n",
        "vectordb = Chroma.from_documents(documents=texts, embedding=embeddings, persist_directory=persist_directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "2BuFWyJoN1H4"
      },
      "outputs": [],
      "source": [
        "vectordb.persist()\n",
        "vectordb = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CB2CJ6FN4dP",
        "outputId": "7d34c3b1-058d-47ac-d985-1af04d15649e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "f:\\shanika\\faye3\\Lib\\site-packages\\langchain\\chains\\retrieval_qa\\base.py:289: UserWarning: `VectorDBQA` is deprecated - please use `from langchain.chains import RetrievalQA`\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import VectorDBQA\n",
        "# Now we can load the persisted database from disk, and use it as normal.\n",
        "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
        "qa = VectorDBQA.from_chain_type(llm=llm, chain_type=\"stuff\", vectorstore=vectordb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"Frame multiple questions and answers from the document\"\n",
        "qa.run(query)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

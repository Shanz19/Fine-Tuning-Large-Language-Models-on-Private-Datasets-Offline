{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfmZ8TnZw3IT"
      },
      "source": [
        "https://youtu.be/nsdCRVuprDY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xwk6OoI2Ep7Z"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "l9PhAQicEyQd"
      },
      "outputs": [],
      "source": [
        "!pip install -U PyPDF2\n",
        "!pip install python-docx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Ly_QfYPDHlie"
      },
      "outputs": [],
      "source": [
        "# Functions to read different file types\n",
        "def read_pdf(file_path):\n",
        "    with open(file_path, \"rb\") as file:\n",
        "        pdf_reader = PdfReader(file)\n",
        "        text = \"\"\n",
        "        for page_num in range(len(pdf_reader.pages)):\n",
        "            text += pdf_reader.pages[page_num].extract_text()\n",
        "    return text\n",
        "\n",
        "def read_word(file_path):\n",
        "    doc = docx.Document(file_path)\n",
        "    text = \"\"\n",
        "    for paragraph in doc.paragraphs:\n",
        "        text += paragraph.text + \"\\n\"\n",
        "    return text\n",
        "\n",
        "def read_txt(file_path):\n",
        "    with open(file_path, \"r\") as file:\n",
        "        text = file.read()\n",
        "    return text\n",
        "\n",
        "def read_documents_from_directory(directory):\n",
        "    combined_text = \"\"\n",
        "    for filename in os.listdir(directory):\n",
        "        file_path = os.path.join(directory, filename)\n",
        "        if filename.endswith(\".pdf\"):\n",
        "            combined_text += read_pdf(file_path)\n",
        "        elif filename.endswith(\".docx\"):\n",
        "            combined_text += read_word(file_path)\n",
        "        elif filename.endswith(\".txt\"):\n",
        "            combined_text += read_txt(file_path)\n",
        "    return combined_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "oobynnecHx87"
      },
      "outputs": [],
      "source": [
        "# Read documents from the directory\n",
        "#train_directory = '/content/drive/MyDrive/ColabNotebooks/data/chatbot_docs/training_data/full_text'\n",
        "train_directory = 'conf_docs'\n",
        "text_data = read_documents_from_directory(train_directory)\n",
        "text_data = re.sub(r'\\n+', '\\n', text_data).strip()  # Remove excess newline characters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8RCPr_L0uJdo"
      },
      "outputs": [],
      "source": [
        "with open(\"conf_docs/train.txt\", \"w\") as f:\n",
        "    f.write(text_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9MFaNgaDEVKP"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/shanzz/Documents/FINETUNE/faye/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/shanzz/Documents/FINETUNE/faye/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
            "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
          ]
        }
      ],
      "source": [
        "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from transformers import Trainer, TrainingArguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Wk_14dI9EVdD"
      },
      "outputs": [],
      "source": [
        "def load_dataset(file_path, tokenizer, block_size = 128):\n",
        "    dataset = TextDataset(\n",
        "        tokenizer = tokenizer,\n",
        "        file_path = file_path,\n",
        "        block_size = block_size,\n",
        "    )\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "QJcoT-aNFcKp"
      },
      "outputs": [],
      "source": [
        "def load_data_collator(tokenizer, mlm = False):\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer,\n",
        "        mlm=mlm,\n",
        "    )\n",
        "    return data_collator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT-2 small model downloaded and saved successfully!\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2Tokenizer, GPT2Model, GPT2Config, AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Define the model name\n",
        "model_name = \"openai-community/gpt2\"\n",
        "\n",
        "# Load tokenizer and model configuration\n",
        "config = AutoConfig.from_pretrained(model_name)\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Load model\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Define the path where you want to save the model\n",
        "save_path = \"./gpt2_small_model\"\n",
        "\n",
        "# Save tokenizer and model to the specified directory\n",
        "tokenizer.save_pretrained(save_path)\n",
        "model.save_pretrained(save_path)\n",
        "\n",
        "print(\"GPT-2 small model downloaded and saved successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_name = '/Users/shanzz/Documents/FINETUNE/311/gpt2_small_model'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Ub3THjJ_FdSw"
      },
      "outputs": [],
      "source": [
        "def train(train_file_path,model_name,\n",
        "          output_dir,\n",
        "          overwrite_output_dir,\n",
        "          per_device_train_batch_size,\n",
        "          num_train_epochs,\n",
        "          save_steps):\n",
        "  tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "  train_dataset = load_dataset(train_file_path, tokenizer)\n",
        "  data_collator = load_data_collator(tokenizer)\n",
        "\n",
        "  tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "  model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "  model.save_pretrained(output_dir)\n",
        "\n",
        "  training_args = TrainingArguments(\n",
        "          output_dir=output_dir,\n",
        "          overwrite_output_dir=overwrite_output_dir,\n",
        "          per_device_train_batch_size=per_device_train_batch_size,\n",
        "          num_train_epochs=num_train_epochs,\n",
        "      )\n",
        "\n",
        "  trainer = Trainer(\n",
        "          model=model,\n",
        "          args=training_args,\n",
        "          data_collator=data_collator,\n",
        "          train_dataset=train_dataset,\n",
        "  )\n",
        "\n",
        "  trainer.train()\n",
        "  trainer.save_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9mXiWKHbFr2f"
      },
      "outputs": [],
      "source": [
        "\n",
        "#train_file_path = \"/content/drive/MyDrive/ColabNotebooks/data/chatbot_docs/combined_text/full_text/train.txt\"\n",
        "train_file_path = \"conf_docs/train.txt\"\n",
        "model_name = 'gpt2_small_model'\n",
        "#output_dir = '/content/drive/MyDrive/ColabNotebooks/models/chat_models/custom_full_text'\n",
        "output_dir = 'output_dir'\n",
        "overwrite_output_dir = False\n",
        "per_device_train_batch_size = 8\n",
        "num_train_epochs = 100\n",
        "save_steps = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "import accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "WMdaTo7KF9uo"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/shanzz/Documents/FINETUNE/faye/lib/python3.11/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n",
            "100%|██████████| 100/100 [02:29<00:00,  1.50s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train_runtime': 149.5001, 'train_samples_per_second': 2.676, 'train_steps_per_second': 0.669, 'train_loss': 0.4458774185180664, 'epoch': 100.0}\n"
          ]
        }
      ],
      "source": [
        "# Train\n",
        "train(\n",
        "    train_file_path=train_file_path,\n",
        "    model_name=model_name,\n",
        "    output_dir=output_dir,\n",
        "    overwrite_output_dir=overwrite_output_dir,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    save_steps=save_steps\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwGS1IMlGBMB"
      },
      "source": [
        "Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "lelq_sN4Gy5M"
      },
      "outputs": [],
      "source": [
        "from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel, GPT2TokenizerFast, GPT2Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "wOvrNQRAG2IP"
      },
      "outputs": [],
      "source": [
        "def load_model(model_path):\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_path)\n",
        "    return model\n",
        "\n",
        "\n",
        "def load_tokenizer(tokenizer_path):\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_path)\n",
        "    return tokenizer\n",
        "\n",
        "def generate_text(model_path, sequence, max_length):\n",
        "\n",
        "    model = load_model(model_path)\n",
        "    tokenizer = load_tokenizer(model_path)\n",
        "    ids = tokenizer.encode(f'{sequence}', return_tensors='pt')\n",
        "    final_outputs = model.generate(\n",
        "        ids,\n",
        "        do_sample=True,\n",
        "        max_length=max_length,\n",
        "        pad_token_id=model.config.eos_token_id,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "    )\n",
        "    print(tokenizer.decode(final_outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model2_path = \"output_dir\"\n",
        "sequence2 = \"Q: What is an Individual Surface Target Message?\"\n",
        "max_length = None\n",
        "generate_text(model2_path, sequence2, max_length)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
